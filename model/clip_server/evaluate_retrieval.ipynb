{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1e4d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19fdacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Setup Transform and Device ====\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61823474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Define Embedding Extractor ====\n",
    "class EfficientNetEmbeddingExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.features = model.features\n",
    "        self.avgpool = model.avgpool\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = model.classifier[0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f12f3517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Load Model ====\n",
    "base_model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "base_model.classifier[1] = nn.Linear(base_model.classifier[1].in_features, 1000)  # dummy\n",
    "# base_model.load_state_dict(torch.load(\"efficientnet_b0_landmark.pth\", map_location=device))\n",
    "base_model.eval()\n",
    "embedding_model = EfficientNetEmbeddingExtractor(base_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69370f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Load Images and Labels ====\n",
    "image_folder = \"../../../images/\"\n",
    "image_paths = []\n",
    "labels = []\n",
    "label_map = {}\n",
    "\n",
    "for label in os.listdir(image_folder):\n",
    "    label_path = os.path.join(image_folder, label)\n",
    "    if not os.path.isdir(label_path): continue\n",
    "    for fname in os.listdir(label_path):\n",
    "        if fname.endswith(\".jpg\"):\n",
    "            path = os.path.join(label_path, fname)\n",
    "            image_paths.append(path)\n",
    "            labels.append(label)\n",
    "            label_map[path] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89fe1a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   0%|          | 0/51 [00:00<?, ?it/s][W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "Extracting embeddings: 100%|██████████| 51/51 [00:04<00:00, 11.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# ==== Compute Embeddings ====\n",
    "embeddings = []\n",
    "for path in tqdm(image_paths, desc=\"Extracting embeddings\"):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    x = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = embedding_model(x)\n",
    "        emb = F.normalize(emb, dim=-1)  # L2 normalize\n",
    "        embeddings.append(emb.squeeze().cpu())\n",
    "\n",
    "embedding_db = torch.stack(embeddings)\n",
    "label_db = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11e062e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 1392.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Top-1 Accuracy: 0.8039\n",
      "✅ Top-5 Accuracy: 0.9412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==== Evaluation Loop ====\n",
    "top1_correct = 0\n",
    "top5_correct = 0\n",
    "n = len(image_paths)\n",
    "\n",
    "print(\"Evaluating retrieval...\")\n",
    "for i, query_emb in enumerate(tqdm(embedding_db)):\n",
    "    true_label = label_db[i]\n",
    "\n",
    "    sims = F.cosine_similarity(query_emb.unsqueeze(0), embedding_db)  # (N,)\n",
    "    sims[i] = -1.0  # exclude self\n",
    "\n",
    "    topk = torch.topk(sims, k=5).indices\n",
    "    topk_labels = [label_db[j] for j in topk]\n",
    "\n",
    "    if topk_labels[0] == true_label:\n",
    "        top1_correct += 1\n",
    "    if true_label in topk_labels:\n",
    "        top5_correct += 1\n",
    "\n",
    "top1_acc = top1_correct / n\n",
    "top5_acc = top5_correct / n\n",
    "\n",
    "print(f\"\\n✅ Top-1 Accuracy: {top1_acc:.4f}\")\n",
    "print(f\"✅ Top-5 Accuracy: {top5_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6682b97c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
