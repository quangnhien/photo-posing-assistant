{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1e4d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NhienLQ/opt/anaconda3/envs/photo-posing-assistant/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPProcessor, CLIPModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19fdacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Setup Transform and Device ====\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61823474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Define Embedding Extractor ====\n",
    "class EfficientNetEmbeddingExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.features = model.features\n",
    "        self.avgpool = model.avgpool\n",
    "        self.flatten = nn.Flatten()\n",
    "        # self.dropout = model.classifier[0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        # x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "825eeb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = models.efficientnet_b0(weights=None)\n",
    "in_features = base_model.classifier[1].in_features\n",
    "base_model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.4),\n",
    "    nn.Linear(in_features, 512),\n",
    "    nn.Dropout(p=0.4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 3103)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70dcd641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights\n",
    "base_model.load_state_dict(torch.load(\"model/efficientnet_b0_landmark.pth\", map_location=torch.device('cpu')))\n",
    "base_model.eval()\n",
    "embedding_model = EfficientNetEmbeddingExtractor(base_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69370f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Load Images and Labels ====\n",
    "image_folder = \"../../../images/\"\n",
    "image_paths = []\n",
    "labels = []\n",
    "label_map = {}\n",
    "\n",
    "for label in os.listdir(image_folder):\n",
    "    label_path = os.path.join(image_folder, label)\n",
    "    if not os.path.isdir(label_path): continue\n",
    "    for fname in os.listdir(label_path):\n",
    "        if fname.endswith(\".jpg\"):\n",
    "            path = os.path.join(label_path, fname)\n",
    "            image_paths.append(path)\n",
    "            labels.append(label)\n",
    "            label_map[path] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89fe1a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   0%|          | 0/51 [00:00<?, ?it/s][W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "Extracting embeddings: 100%|██████████| 51/51 [00:05<00:00,  9.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# ==== Compute Embeddings ====\n",
    "embeddings = []\n",
    "for path in tqdm(image_paths, desc=\"Extracting embeddings\"):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    x = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = embedding_model(x)\n",
    "        emb = F.normalize(emb, dim=-1)  # L2 normalize\n",
    "        embeddings.append(emb.squeeze().cpu())\n",
    "\n",
    "embedding_db = torch.stack(embeddings)\n",
    "label_db = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11e062e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 2289.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Top-1 Accuracy: 0.8039\n",
      "✅ Top-5 Accuracy: 0.9216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==== Evaluation Loop ====\n",
    "top1_correct = 0\n",
    "top5_correct = 0\n",
    "n = len(image_paths)\n",
    "\n",
    "print(\"Evaluating retrieval...\")\n",
    "for i, query_emb in enumerate(tqdm(embedding_db)):\n",
    "    true_label = label_db[i]\n",
    "\n",
    "    sims = F.cosine_similarity(query_emb.unsqueeze(0), embedding_db)  # (N,)\n",
    "    sims[i] = -1.0  # exclude self\n",
    "\n",
    "    topk = torch.topk(sims, k=5).indices\n",
    "    topk_labels = [label_db[j] for j in topk]\n",
    "\n",
    "    if topk_labels[0] == true_label:\n",
    "        top1_correct += 1\n",
    "    if true_label in topk_labels:\n",
    "        top5_correct += 1\n",
    "\n",
    "top1_acc = top1_correct / n\n",
    "top5_acc = top5_correct / n\n",
    "\n",
    "print(f\"\\n✅ Top-1 Accuracy: {top1_acc:.4f}\")\n",
    "print(f\"✅ Top-5 Accuracy: {top5_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33b8b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\",use_safetensors=True).to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbc2d20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 51/51 [00:11<00:00,  4.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# ==== Compute Embeddings ====\n",
    "embeddings = []\n",
    "for path in tqdm(image_paths, desc=\"Extracting embeddings\"):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    x = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = clip_model.get_image_features(**x)\n",
    "        emb = F.normalize(emb, dim=-1)  # L2 normalize\n",
    "        embeddings.append(emb.squeeze().cpu())\n",
    "\n",
    "embedding_db = torch.stack(embeddings)\n",
    "label_db = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc0bcb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 6678.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Top-1 Accuracy: 0.9020\n",
      "✅ Top-5 Accuracy: 0.9412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==== Evaluation Loop ====\n",
    "top1_correct = 0\n",
    "top5_correct = 0\n",
    "n = len(image_paths)\n",
    "\n",
    "print(\"Evaluating retrieval...\")\n",
    "for i, query_emb in enumerate(tqdm(embedding_db)):\n",
    "    true_label = label_db[i]\n",
    "\n",
    "    sims = F.cosine_similarity(query_emb.unsqueeze(0), embedding_db)  # (N,)\n",
    "    sims[i] = -1.0  # exclude self\n",
    "\n",
    "    topk = torch.topk(sims, k=5).indices\n",
    "    topk_labels = [label_db[j] for j in topk]\n",
    "\n",
    "    if topk_labels[0] == true_label:\n",
    "        top1_correct += 1\n",
    "    if true_label in topk_labels:\n",
    "        top5_correct += 1\n",
    "\n",
    "top1_acc = top1_correct / n\n",
    "top5_acc = top5_correct / n\n",
    "\n",
    "print(f\"\\n✅ Top-1 Accuracy: {top1_acc:.4f}\")\n",
    "print(f\"✅ Top-5 Accuracy: {top5_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163cc08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "photo-posing-assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
